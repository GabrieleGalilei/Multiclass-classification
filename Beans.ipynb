{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d25f0b",
   "metadata": {},
   "source": [
    "# Analisi Dataset - DryBeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549f92a",
   "metadata": {},
   "source": [
    "Lo scopo della seguente trattazione è approfondire l'applicazione di alcuni dei metodi studiati durante il corso di matematica per l'Intelligenza Artificiale ad un insieme di dati reale, per poter analizzare in modo diverso il comportamento delle variabili. La scelta è ricaduta su un dataset contenente dati riguardo ad alcune specie di fagiolo in forma secca. Su tale dataset si svolgeranno vari algoritmi di analisi e classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207c5819",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18460\\3628842588.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display\n",
    "from linear_r2 import generate_square, HyperplaneR2\n",
    "from FisherDA import MultipleFisherDiscriminantAnalysis as MDA\n",
    "import cycler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563f083",
   "metadata": {},
   "source": [
    "## Importazione del dataset\n",
    "Il dataset è stato creato da:\n",
    "\n",
    "Murat KOKLU\n",
    "Faculty of Technology,\n",
    "Selcuk University,\n",
    "TURKEY.\n",
    "ORCID : 0000-0002-2737-2360\n",
    "mkoklu@selcuk.edu.tr\n",
    "\n",
    "e\n",
    "\n",
    "Ilker Ali OZKAN\n",
    "Faculty of Technology,\n",
    "Selcuk University,\n",
    "TURKEY.\n",
    "ORCID : 0000-0002-5715-1040\n",
    "ilkerozkan@selcuk.edu.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9582c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans = pd.read_csv('datasets/beans/Dry_Bean_Dataset.csv', header=None)\n",
    "col_names = beans.loc[0,:]\n",
    "beans.drop(0, inplace=True)\n",
    "rename_dict = {k: col_names[k] for k in range(17)}\n",
    "beans.rename(columns=rename_dict, inplace=True)\n",
    "beans_show = pd.DataFrame(beans)\n",
    "beans_show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c070bd",
   "metadata": {},
   "source": [
    "Come possiamo vedere, il dataset contiene immagini ad alta risoluzione di 13.611 chicci di 7 diverse specie di fagiolo registrate. Ogni chicco è classificato attraverso 16 attributi, 12 riguardanti la dimensione e 4 riguardanti la forma.\n",
    "\n",
    "Attributi:\n",
    "1) **Area (A)**: L'area del fagiolo e numero di pixel interni al suo perimetro.\n",
    "2) **Perimeter (P)**: Circonferenza del fagiolo definita come la lunghezza del suo bordo.\n",
    "3) **Major axis length (L)**: Il segmento di lunghezza massima tra due punti del bordo del fagiolo\n",
    "4) **Minor axis length (l)**: Il segmento di lunghezza massima tra due punti del bordo del fagiolo tra quelle perpendicolari a Major axis.\n",
    "5) **Aspect ratio (K)**: Defines the relationship between L and l.\n",
    "6) **Eccentricity (Ec)**: Eccentricità dell'ellisse avente gli stessi momenti della regione.\n",
    "7) **Convex area (C)**: Numero di pixel del più piccolo poligono convesso che circoscrive il fagiolo.\n",
    "8) **Equivalent diameter (Ed)**: Il diametro del cerchio avente stessa area del fagiolo.\n",
    "9) **Extent (Ex)**: Il rapporto tra i pixel dell'area del fagiolo e i pixel totali dell'inquadratura.\n",
    "10) **Solidity (S)**: Anche detta convessità. Il rapporto tra i pixel nell'area del fagiolo e quelli nel guscio convesso.\n",
    "11) **Roundness (R)**: Calcolato con la seguente formula: $\\frac{4\\pi A}{P^2}$\n",
    "12) **Compactness (CO)**: Misura della rotondità del fagiolo: $\\frac{Ed}{L}$\n",
    "13) **ShapeFactor1 (SF1)**\n",
    "14) **ShapeFactor2 (SF2)**\n",
    "15) **ShapeFactor3 (SF3)**\n",
    "16) **ShapeFactor4 (SF4)**\n",
    "17) **Class**: le 7 diverse specie di fagiolo: Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira\n",
    "\n",
    "## Analisi del database\n",
    "\n",
    "Chiamiamo M e N le dimensioni del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans = beans.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = beans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e92800",
   "metadata": {},
   "source": [
    "Stampiamo a video alcune informazioni sul database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ef20f",
   "metadata": {},
   "source": [
    "Da queste possiamo vedere, come già dichiarato nel README, che il database non ha dati in Null e quindi è completo.\n",
    "\n",
    "Ora andiamo a riportare il numero di istanze per classe e la frequenza di ogni classe sul totale. Successivamente stampiamo a video un istogramma che riporta le frequenze delle classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acdbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_cont_freq = pd.concat([beans['Class'].value_counts(), beans['Class'].value_counts()/M], axis=1)\n",
    "class_cont_freq.columns = ['counts', 'freq.']\n",
    "class_cont_freq.index.name = 'Class'\n",
    "\n",
    "display(class_cont_freq);\n",
    "beans['Class'].value_counts().plot.bar(figsize=(10, 6), color=[(68/235,99/235,56/235)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302bd7d",
   "metadata": {},
   "source": [
    "Possiamo quindi vedere come, almeno nella popolazione studiata dal dataset, la specie più comune è la Dermason e la meno comune è la Bombay.\n",
    "\n",
    "## Label Encoder\n",
    "Per i prossimi algoritmi si crea un database di copia sul quale operare. Successivamente si cambia il datatype dell'attributo classe da char a int per poter eseguire algoritmi numerici sul dataset. Per far questo usiamo il Label Encoder. Nel nostro caso abbiamo 7 classi che saranno classificate dal LE come gli interi da 0 a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54995caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beans_work = beans.sample(3000).copy()\n",
    "beans_work = beans.copy()\n",
    "beans_original = beans.copy()\n",
    "beans = beans_work\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(beans['Class'])\n",
    "beans['Class']=le.transform(beans['Class'])\n",
    "Beans_codes = {'SEKER':0,'BARBUNYA':1, 'BOMBAY':2, 'CALI':3, 'HOROZ':4, 'SIRA':5, 'DERMASON':6} #dizionario creato per visualizzare le legende"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a594f",
   "metadata": {},
   "source": [
    "## Controlli: Normality Test e Homoscedasticity Test\n",
    "\n",
    "Controlliamo ora che al dataset in analisi siano applicabili i metodi di classificazione studiati. Analizziamo prima la normalità. Essenso il dataset abbastanza grande l'attesa è di p-value molto bassi, purtroppo. Infatti riteniamo accettabile l'ipotesi di normalità per p-value superiori a 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_beans = beans.iloc[:, :-1]\n",
    "scaler_beans = StandardScaler()\n",
    "scaler_beans.fit(X_beans.values)\n",
    "X_beans_scaled = scaler_beans.transform(X_beans.values)\n",
    "beans_scaled_use = pd.DataFrame(X_beans_scaled, columns=col_names[:-1])\n",
    "\n",
    "normality = pd.DataFrame(index=['K-value','p-value'], columns=col_names[:-1])\n",
    "for i in range(16):\n",
    "    out = sp.stats.normaltest(beans_scaled_use.iloc[:,i])\n",
    "    normality.iloc[0,i] = out[0]\n",
    "    normality.iloc[1,i] = out[1]\n",
    "normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5430e9",
   "metadata": {},
   "source": [
    "Come prospettato il normality test non ha dato i risultati sperati; quindi si passa ad una visualizzazione diretta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest = plt.figure(figsize=(20, 15))\n",
    "for j in range (len(col_names)-1):\n",
    "    normaltest.add_subplot(4,4,j+1)\n",
    "    mu, std = sp.stats.norm.fit(beans_scaled_use.iloc[:,j]) \n",
    "\n",
    "    plt.hist(beans_scaled_use.iloc[:,j], bins=20, density=True, alpha=1, color=[(68/235,99/235,56/235)])\n",
    "\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax)\n",
    "    p = sp.stats.norm.pdf(x, mu, std)\n",
    "\n",
    "    plt.plot(x, p, 'orange', linewidth=2)\n",
    "    title = str(col_names[j]).format(mu, std)\n",
    "    plt.grid()\n",
    "    plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0460d5",
   "metadata": {},
   "source": [
    "Possiamo quindi vedere come gli attributi siano distribuiti quasi normalmente e quindi possiamo ritenerli accettabili. Lo stesso discorso può essere fatto per l'omoschedasticità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce315bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outhom = sp.stats.levene(beans_scaled_use.iloc[:,0], beans_scaled_use.iloc[:,1], beans_scaled_use.iloc[:,2],\n",
    "                         beans_scaled_use.iloc[:,3], beans_scaled_use.iloc[:,4], beans_scaled_use.iloc[:,5],\n",
    "                         beans_scaled_use.iloc[:,6], beans_scaled_use.iloc[:,7], beans_scaled_use.iloc[:,8],\n",
    "                         beans_scaled_use.iloc[:,9], beans_scaled_use.iloc[:,10], beans_scaled_use.iloc[:,11],\n",
    "                         beans_scaled_use.iloc[:,12], beans_scaled_use.iloc[:,13], beans_scaled_use.iloc[:,14],\n",
    "                         beans_scaled_use.iloc[:,15], center='mean')\n",
    "\n",
    "homoscedasticity = pd.DataFrame(index=['K-value','p-value'], columns=['Result'])\n",
    "homoscedasticity.iloc[0,0] = outhom[0]\n",
    "homoscedasticity.iloc[1,0] = outhom[1]\n",
    "homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf224b",
   "metadata": {},
   "source": [
    "***DISCLAIMER***: ho provato ad usare Wine Quality (sia rosso che bianco), Page blocks, Magic, Hepatitis C Virus e Dry Beans, ma in tutti i casi i test davano esito negativo. Ho deciso quindi di usare questo dataset in quanto quello che \"rispetta\" maggiormente le condizioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f215a",
   "metadata": {},
   "source": [
    "## PCA - Principal Component Analysis\n",
    "\n",
    "La PCA è un algoritmo di data representation non supervisionato che ha come obiettivo l'individuazione, nell'iperspazio degli attributi, delle direzioni in cui i dati presentano la massima varianza per la riduzione della dimensionalità del dataset d-dimensionale proiettandolo in un sottospazio k-dimensioanle (con $k<d$).\n",
    "È lecito chiedersi quale sia un valore di k tale che si ottenga una buona mole di informazioni. Analizziamo l'algoritmo per step:\n",
    "- Standardizzazione dei dati (std = 1, mean = 0)\n",
    "- calcolo degli autovalori e dei relativi autovettori della matrice di covarianza\n",
    "- Disposizione decrescente degli autovalori e selezione dei primi k autovettori (associati quindi ai k autovalori più grandi) dove k è la nuova dimensionalità\n",
    "- Costruzione della projection matrix W\n",
    "- Trasformazione del dataset tramite la matrice W per ottenere il nuovo sottospazio\n",
    "\n",
    "Quindi al dataset vengono rimossi i dati relativi alla classe, cioè la colonna dei target, e successivmente lo **Standard Scaler** normalizza le colonne in modo tale che queste abbiano media 0 e varianza unitaria. Questo passaggio è fondamentale perchè nel dataset in analisi gli attributi hanno unità di misura non paragonabili e ampiezza del sottocampione molto diversa.\n",
    "\n",
    "Si procede calcolando autovalori e autovettori della matrice di covarianza. Si ordinano in modo decrescente gli autovalori: questi rappresentano la quantità di varianza, detta **varianza spiegata**, nella direzione del corrispondente autovettore, detto **compenente principale**.\n",
    "\n",
    "### PCA: Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si ricords che lo scaler è già stato usato nei controlli\n",
    "pca_beans = PCA()\n",
    "pca_beans_nostd = PCA()\n",
    "\n",
    "pca_beans.fit(X_beans_scaled)\n",
    "pca_beans_nostd.fit(X_beans.values)\n",
    "\n",
    "fig_stand = plt.figure(figsize=(15, 4));\n",
    "nostand = fig_stand.add_subplot(1, 2, 1);\n",
    "plt.plot(np.insert(np.cumsum(pca_beans_nostd.explained_variance_ratio_), 0, 0), color=(68/235,99/235,56/235))\n",
    "nostand.set_title('\\nBEANS (NO STANDARDIZATION)\\n')\n",
    "plt.xticks(ticks=np.arange(1, pca_beans_nostd.n_features_ + 1), \n",
    "           labels=[f'PC{i}' for i in range(1, pca_beans_nostd.n_features_ + 1)])\n",
    "plt.xlabel('\\nPrincipal components\\n')\n",
    "plt.ylabel('\\nCumulative explained variance\\n')\n",
    "plt.grid()\n",
    "\n",
    "stand = fig_stand.add_subplot(1, 2, 2);\n",
    "plt.plot(np.insert(np.cumsum(pca_beans.explained_variance_ratio_), 0, 0), color=(68/235,99/235,56/235))\n",
    "stand.set_title('\\nBEANS (WITH STANDARDIZATION)\\n')\n",
    "plt.xticks(ticks=np.arange(1, pca_beans.n_features_ + 1), \n",
    "           labels=[f'PC{i}' for i in range(1, pca_beans.n_features_ + 1)])\n",
    "plt.xlabel('Principal components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12212a7",
   "metadata": {},
   "source": [
    "Da questo grafico si vede chiaramente come bastino sole due componenti principali per spiegare più dell'80% della varianza totale; per questo uno spazio bidimensionale è più che sufficiente per visualizzare la separazione delle classi. In 3 dimensioni la varianza spiegata cumulata arriva a circa il 90% ed è questo l'esempio riportato.\n",
    "\n",
    "### PCA: Score Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_beans_m = PCA(n_components=3)\n",
    "pca_beans_m.fit(X_beans_scaled)\n",
    "Y_beans_m = pca_beans_m.transform(X_beans_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_beansscore = plt.figure(figsize=[10,10])\n",
    "ax = fig_beansscore.add_subplot(111, projection='3d')\n",
    "plt.set_cmap('Accent')\n",
    "scatter = ax.scatter(Y_beans_m[:, 0], Y_beans_m[:, 1], Y_beans_m[:, 2], c=beans['Class'].values, alpha=1.00)\n",
    "ax.legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), bbox_to_anchor=(1.05, 1), fontsize='large', title=\"Classes\")\n",
    "plt.title('\\nBEANS - SCORE GRAPH\\n')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "scatter.set_alpha(0.15);\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47486716",
   "metadata": {},
   "source": [
    "### PCA: Loading Graph\n",
    "\n",
    "Resta da vedere come gli attributi siano legati alle varie componenti principali e quindi capire quali attributi siano i più caratterizzanti o meglio \"classificanti\". Per questo si usa il **Loading Graph**. Più un vettore relativo ad un attributo è parallelo alla PC1 più questo pesa sulla classificazione: in questo caso lo ShapeFactor1 è quello più caratterizzante.\n",
    "Inoltre, più l'angolo tra due vettori è piccolo (ca. 0) più gli attributi relativi sono correlati positivamente (come ConvexArea e EquivDiameter), più è grande (ca. $\\pi$) più gli attributi relativi sono correlati negativamente (come per AspectRation e ShapeFactor3), più sono vicini all'ortogonalità (ca. $\\frac{\\pi}{2}$) più sono vicini alla scorrelazione (come per Perimeter e Compactness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10];\n",
    "fig_beansscore = plt.figure();\n",
    "ax = fig_beansscore.add_subplot(111, projection='3d');\n",
    "for i in range(pca_beans_m.n_features_):\n",
    "    ax.plot([0, pca_beans_m.components_[0, i]], [0, pca_beans_m.components_[1, i]], [0, pca_beans_m.components_[2, i]], label=X_beans.columns[i]);\n",
    "ax.scatter(pca_beans_m.components_[0, :], pca_beans_m.components_[1, :], pca_beans_m.components_[2, :], c='k');\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), fontsize='large');\n",
    "plt.title('\\nBEANS - LOADING GRAPH\\n');\n",
    "ax.set_xlabel('PC1');\n",
    "ax.set_ylabel('PC2');\n",
    "ax.set_zlabel('PC3');\n",
    "plt.grid();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3789d1f",
   "metadata": {},
   "source": [
    "Ora mostriamo un'interpretazione più chiara del loading graph che ci permette di vedere le componenti di ogni attributo per ogni componente principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = plt.figure(figsize=(21, 4));\n",
    "pc1_comp = components.add_subplot(1, 3, 1);\n",
    "plt.bar(np.arange(pca_beans_m.n_features_), pca_beans_m.components_[0, :], color=[(68/235,99/235,56/235)])\n",
    "plt.xticks(ticks=np.arange(pca_beans_m.n_features_), \n",
    "           labels=X_beans.columns.to_list(),\n",
    "           rotation=90)\n",
    "plt.title('\\nBEANS - PC1\\n')\n",
    "plt.grid()\n",
    "\n",
    "pc1_comp = components.add_subplot(1, 3, 2);\n",
    "plt.bar(np.arange(pca_beans_m.n_features_), pca_beans_m.components_[1, :], color=[(68/235,99/235,56/235)])\n",
    "plt.xticks(ticks=np.arange(pca_beans_m.n_features_), \n",
    "           labels=X_beans.columns.to_list(),\n",
    "           rotation=90)\n",
    "plt.title('\\nBEANS - PC2\\n')\n",
    "plt.grid()\n",
    "\n",
    "pc3_comp = components.add_subplot(1, 3, 3);\n",
    "plt.bar(np.arange(pca_beans_m.n_features_), pca_beans_m.components_[2, :], color=[(68/235,99/235,56/235)])\n",
    "plt.xticks(ticks=np.arange(pca_beans_m.n_features_), \n",
    "           labels=X_beans.columns.to_list(),\n",
    "           rotation=90)\n",
    "plt.title('\\nBEANS - PC2\\n')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13e92c",
   "metadata": {},
   "source": [
    "Da qui possiamo vedere che mentre la PC1 sembra essere legata alla dimensione del fagiolo, la PC2 e la PC3 sembrano correlate alla sua forma, in particolare alla sua rootondità.\n",
    "Quindi possiamo dare i seguenti nomi:\n",
    "- PC1 : Dimension\n",
    "- PC2 : Roundness1\n",
    "- PC3 : Roundness2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66dd32b",
   "metadata": {},
   "source": [
    "## MDA - Multiple Fischer Discriminant Analysis\n",
    "\n",
    "Si parla di MDA come della versione multiclasse della FDA (Fischer Discriminant Analysis): sono algoritmi supervisionati di data classification. Nel caso in analisi si hanno 7 classi quindi la MDA può ridurre la dimensionalità fino a 6. Scopo della MDA è appunto quello di proiettare i dati in uno spazio di dimensione minore massimizzando lo spazio tra una classe e l'altra (tramite l'uso della beetween scatter matrix) e minimizzando lo spazio tra elementi della stessa classe (within scatter matrix).\n",
    "\n",
    "### MDA: basi teoriche\n",
    "\n",
    "Sia $\\{x_i\\}_{i=1}^n$ l'insieme dei sample divisi in $c$ classi con cardinalità $n_i$. Sia $V$ la matrice di proiezione. Allora i punti proiettati sono $y_i=V^Tx_i$.\n",
    "\n",
    "Media totale: $\\mu=\\frac{1}{n}\\sum_{i=1}^n x_i$\n",
    "\n",
    "Media della classe j: $\\mu_j=\\frac{1}{n_j}\\sum_{x_i\\in class j} x_i$\n",
    "\n",
    "Media totale delle proiezioni: $\\tilde{\\mu}=\\frac{1}{n}\\sum_{i=1}^n y_i$\n",
    "\n",
    "Media delle proiezioni della classe j: $\\tilde{\\mu}_j=\\frac{1}{n_j}\\sum_{x_i\\in class j} y_i$\n",
    "\n",
    "Definiamo la **Within Scatter Matrix** come:\n",
    "\\begin{equation*}\n",
    "    S_W=\\sum_{j=1}^c [\\sum_{x_k\\in\\;class\\;j}(x_k-\\mu_i)(x_k-\\mu_i)^T]\n",
    "\\end{equation*}\n",
    "\n",
    "Definiamo la **Between Scatter Matrix** come:\n",
    "\\begin{equation*}\n",
    "    S_B=\\sum_{j=1}^c n_j (\\mu_i-\\mu)(\\mu_i-\\mu)^T\n",
    "\\end{equation*}\n",
    "\n",
    "Si dimostra che la proiezione voluta V è tale che massimizza la funzione obiettivo:\n",
    "\\begin{equation*}\n",
    "    J(V)=\\frac{det(V^TS_BV)}{det(V^TS_WV)}.\n",
    "\\end{equation*}\n",
    "\n",
    "Da qui si dimostra che basta risolvere il problema agli autovalori generalizzati:\n",
    "\\begin{equation*}\n",
    "    S_Bv=\\lambda S_W v\n",
    "\\end{equation*}\n",
    "dal quale ricaviamo al massimo c-1 autovalori e corrispondenti autovettori. Ordiniamo gli autovalori in ordine descrescente e prendendo i k autovalori più grandi, i relativi autovettori formeranno la matrice di proiezione cercata su uno spazio k-dimensionale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5af82",
   "metadata": {},
   "source": [
    "### MDA: Visualizzazione e confronto con PCA\n",
    "\n",
    "Si visualizza dunque l'MDA a confronto con la PCA in 1, 2, 3 dimensioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione degli oggetti MultipleFisherDiscriminantAnalysis\n",
    "mda_3dim = MDA(n_dimensions=3)  # Per la proiezione su 3 dimensioni\n",
    "mda_2dim = MDA(n_dimensions=2)  # Per la proiezione su 2 dimensioni\n",
    "mda_1dim = MDA(n_dimensions=1)  # Per la proiezione su una dimensione\n",
    "\n",
    "# Inizializzazione degli oggetti PrincipalComponentAnalysis\n",
    "pca_3dim = PCA(n_components=3)  # Per la proiezione su 3 dimensioni\n",
    "pca_2dim = PCA(n_components=2)  # Per la proiezione su 2 dimensioni\n",
    "pca_1dim = PCA(n_components=1)  # Per la proiezione su una dimensione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparazione dataset per i metodi \"fit\" di mda_1dim, mda_2dim, pca_1dim, pca_2dim.\n",
    "X = X_beans_scaled;\n",
    "y = beans['Class'].values;\n",
    "#beans['Class'] = le.inverse_transform(beans['Class'])  fmt=['SEKER', 'BARBUNYA' 'BOMBAY', 'CALI', 'HOROZ', 'SIRA', 'DERMASON']\n",
    "\n",
    "mda_3dim.fit(X_beans_scaled, y);\n",
    "mda_2dim.fit(X_beans_scaled, y);\n",
    "mda_1dim.fit(X_beans_scaled, y);\n",
    "\n",
    "pca_3dim.fit(X_beans_scaled);\n",
    "pca_2dim.fit(X_beans_scaled);\n",
    "pca_1dim.fit(X_beans_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazione del dataset X rispetto alle proiezioni eseguite da mda_1dim, mda_2dim, pca_1dim, pca_2dim.\n",
    "Z3m = mda_3dim.transform(X_beans_scaled)  # Trasformazione rispetto mda_3dim\n",
    "Z2m = mda_2dim.transform(X_beans_scaled)  # Trasformazione rispetto mda_2dim\n",
    "Z1m = mda_1dim.transform(X_beans_scaled)  # Trasformazione rispetto mda_1dim\n",
    "Z3p = pca_3dim.transform(X_beans_scaled)  # Trasformazione rispetto pca_3dim\n",
    "Z2p = pca_2dim.transform(X_beans_scaled)  # Trasformazione rispetto pca_2dim\n",
    "Z1p = pca_1dim.transform(X_beans_scaled)  # Trasformazione rispetto pca_1dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4adc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per proiezione in R^3\n",
    "fig3 = plt.figure(figsize=(22.6, 8.475));\n",
    "ax3m = fig3.add_subplot(1, 2, 1, projection='3d');\n",
    "scatter = ax3m.scatter(Z3m[:, 0], Z3m[:, 1], Z3m[:, 2], c=y, alpha=1.00);\n",
    "ax3m.set_title('\\n MDA\\n');\n",
    "ax3m.legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "scatter.set_alpha(0.15);\n",
    "ax3p = fig3.add_subplot(1, 2, 2, projection='3d');\n",
    "scatter = ax3p.scatter(Z3p[:, 0], Z3p[:, 1], Z3p[:, 2], c=y, alpha=1.00);\n",
    "ax3p.set_title('\\n PCA\\n');\n",
    "ax3p.legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "scatter.set_alpha(0.15);\n",
    "\n",
    "# Plot per proiezione in R^2\n",
    "fig2, axs2 = plt.subplots(1, 2, figsize=(20, 7.5));\n",
    "scatter = axs2[0].scatter(Z2m[:, 0], Z2m[:, 1], c=y, alpha=1.00);\n",
    "axs2[0].set_title('\\n MDA\\n');\n",
    "axs2[0].legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "scatter.set_alpha(0.15);\n",
    "scatter = axs2[1].scatter(Z2p[:, 0], Z2p[:, 1], c=y, alpha=1.00);\n",
    "axs2[1].set_title('\\n PCA\\n');\n",
    "axs2[1].legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "scatter.set_alpha(0.15);\n",
    "\n",
    "# Plot per proiezione in R^1\n",
    "fig1, axs1 = plt.subplots(1, 2, figsize=(20, 7.5));\n",
    "scatter = axs1[0].scatter(Z1m, Z1m, c=y, alpha=1.00);\n",
    "axs1[0].set_title('\\n MDA\\n');\n",
    "axs1[0].legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "scatter.set_alpha(0.15);\n",
    "\n",
    "scatter = axs1[1].scatter(Z1p, Z1p, c=y, alpha=1.00);\n",
    "axs1[1].set_title('\\n PCA\\n');\n",
    "axs1[1].legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "scatter.set_alpha(0.15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160fe232",
   "metadata": {},
   "source": [
    "## LDA - Linear Discriminant Analysis\n",
    "\n",
    "L'LDA è un algoritmo supervisionato di classificazione di nuovi dati che quindi richiede l'uso di un training set e di un test set. \n",
    "\n",
    "### LDA: basi teoriche\n",
    "Questo si basa sull'analisi distinta di ciascun predittore X su ciascuna delle classi target k tramite la formula di Bayes:\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(Y=k\\vert X=x)=\\frac{\\mathbb{P}(X=x\\vert Y=k)}{\\mathbb{P}(X=x)}.\n",
    "\\end{equation*}\n",
    "Possiamo riscriverlo come:\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(Y=k\\vert X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{p=1}^k \\pi_p f_p(x)}\n",
    "\\end{equation*}\n",
    "dove:\n",
    "- $f_k(x)=\\mathbb{P}(X=x\\vert Y=k)$ è la densità di X nella classe k;\n",
    "- $\\pi_k=\\mathbb{P}(Y=k)$ è la probabilità marginale (o a priori) della classe k.\n",
    "\n",
    "In generale si userà il concetto della **highest density**, cioè ogni istanza verrà asseganta alla classe alle quale corrisponde la più alta densità di probabilità.\n",
    "\n",
    "In tal senso ha senso definire dei **decision boundaries**, cioè degli iperpiani, relativi ad una singola classe (One-vs-Rest), che divino lo spazio in due iperspazi, uno di appartenenza alla classe e uno di non appartenenza.\n",
    "\n",
    "Anche in questo caso è richiesta omoschedasticità e distribuzione \"quasi\" normale. In tali condizioni risulta, utilizzando la notazione introdotta in precedenza:\n",
    "\\begin{equation*}\n",
    "    f_k(x)=\\frac{1}{(2\\pi)^{k/2} det(\\Sigma)^{1/2}}\\,\\exp\\Big(-\\frac{1}{2}\\,(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)\\Big)\n",
    "\\end{equation*}\n",
    "dove con $\\Sigma$ si intemde la matrice di covarianza del dataset.\n",
    "Possiamo sostituire quest'ultima nella formula di Bayes; passando poi al logaritmo (funzione monotona strettamente crescente --> non cambia massimi e minimi) otteniamo il **Discriminant Score**:\n",
    "\\begin{equation*}\n",
    "    \\delta_k(x)=x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log{\\pi_k},\n",
    "\\end{equation*}\n",
    "che come possiamo notare è una funzione lineare di x.\n",
    "Massimizzare la densità di probabilità cercata significa quindi massimizzare il discriminant score. Si definiscono quindi i **Bayes Desicion Boundaries** come le curve lungo le quali i determinant score di due classi confinanti si eguagliano.\n",
    "\n",
    "In primo luogo, dividiamo il dataset in training set e test set attraverso la funzione **train_test_split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3645da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 20210422  # Random seed caratterizzante la suddivisione in training e test set\n",
    "test_p = 0.45  # Percentuale di dati da utilizzare come test set\n",
    "\n",
    "X = X_beans_scaled\n",
    "y = beans['Class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_p, random_state=random_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcc522",
   "metadata": {},
   "source": [
    "Quindi, attraverso il training set, stimiamo $\\pi_k$, $\\mu_k$, $\\Sigma$ (metodo: fit) per ottenere una stima del discriminant score e quindi della denisità di probabilità:\n",
    "\\begin{equation*}\n",
    "    \\hat{\\mathbb{P}}(Y=k\\vert X=x)=\\frac{\\exp(\\hat{\\mu}_k(x))}{\\sum_{i=1}^K \\exp(\\hat{\\mu}_i(x))}.\n",
    "\\end{equation*}\n",
    "\n",
    "Successivamente applichiamo il modello cercato al test set (metodo: predict).\n",
    "\n",
    "### LDA: Accuracy e Confusion Matrix\n",
    "Visualizziamo i risultati ottenuti e la loro precisione tramite accuracy score e confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfea960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "y_pred_proba = lda.predict_proba(X_test)\n",
    "\n",
    "y_pred_df = pd.DataFrame({'Pred. Class': y_pred, \n",
    "                          'P(Class 0) - %': np.round(y_pred_proba[:, 0] * 100, decimals=2), \n",
    "                          'P(Class 1) - %': np.round(y_pred_proba[:, 1] * 100, decimals=2), \n",
    "                          'P(Class 2) - %': np.round(y_pred_proba[:, 2] * 100, decimals=2),\n",
    "                          'P(Class 3) - %': np.round(y_pred_proba[:, 3] * 100, decimals=2), \n",
    "                          'P(Class 4) - %': np.round(y_pred_proba[:, 4] * 100, decimals=2), \n",
    "                          'P(Class 5) - %': np.round(y_pred_proba[:, 5] * 100, decimals=2),\n",
    "                          'P(Class 6) - %': np.round(y_pred_proba[:, 6] * 100, decimals=2)}) # a scopo estetico\n",
    "\n",
    "scores_dict = {'Training Set': lda.score(X_train, y_train), 'Test Set': lda.score(X_test, y_test)}\n",
    "scores = pd.DataFrame(scores_dict, index=['Accuracy'])\n",
    "\n",
    "display(scores)\n",
    "display(y_pred_df)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm=pd.DataFrame(cm)\n",
    "\n",
    "labels = le.classes_\n",
    "class_names = labels\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8.5))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g', cmap='Greens_r'); #annot=True to annotate cells\n",
    "ax.set_xlabel('\\nPredicted\\n', fontsize=10)\n",
    "ax.xaxis.set_label_position('bottom')\n",
    "plt.xticks(rotation=90)\n",
    "ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "ax.xaxis.tick_bottom()\n",
    "\n",
    "ax.set_ylabel('\\n True\\n', fontsize=10)\n",
    "ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.title('\\n Refined Confusion Matrix\\n', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08548f61",
   "metadata": {},
   "source": [
    "Possiamo vedere come la precisione del test set sia abbastanza alta, di circa il 91%, valore più che accettabile.\n",
    "\n",
    "### LDA: Visualizzazione e confronto con MDA\n",
    "Passiamo ora alla rappresentazione. Secondo lo stesso metodo già applicato vediamo in quante dimensioni si ha un livello di varianza cumulativa spiegata sufficiente ad una corretta visualizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_beans = beans.iloc[:, :-1]  # Escludo l'ultima colonna dei target\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4));\n",
    "plt.plot(np.insert(np.cumsum(lda.explained_variance_ratio_), 0, 0), color=(68/235,99/235,56/235))\n",
    "plt.xticks(ticks=np.arange(1, 7), \n",
    "           labels=[f'LD{i}' for i in range(1, 7)])\n",
    "plt.xlabel('\\n Principal components\\n')\n",
    "plt.ylabel('\\n Cumulative explained variance\\n')\n",
    "plt.title('\\n CUMULATIVE EXPLAINED VARIANCE - LDA\\n')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b7069",
   "metadata": {},
   "source": [
    "Possiamo vedere come in 2 dimensioni abbiamo più del 70% di varianza totale, quindi accettabile.\n",
    "\n",
    "Proseguiamo quindi visualizzando i risultati della LDA e confrontandoli con quelli della MDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c897de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mda = MDA()\n",
    "mda.fit(X_train, y_train) # l'ho fatta solo sul training set, per eseere comparabile con la LDA.\n",
    "\n",
    "Zmda = mda.transform(X)\n",
    "Zlda = lda.transform(X)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 7.5));\n",
    "scatter = axs[0].scatter(Zmda[:, 0], Zmda[:, 1], c=y, alpha=1.00);\n",
    "axs[0].legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "axs[0].set_title('\\n MDA\\n');\n",
    "scatter.set_alpha(0.075)\n",
    "plt.grid()\n",
    "scatter = axs[1].scatter(Zlda[:, 0], Zlda[:, 1], c=y, alpha=1.00);\n",
    "axs[1].legend(handles=scatter.legend_elements()[0], labels=Beans_codes.keys(), fontsize='large', title=\"Classes\");\n",
    "axs[1].set_title('\\n LDA\\n');\n",
    "scatter.set_alpha(0.075)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61ee7f",
   "metadata": {},
   "source": [
    "### LDA: Decision Boundaries OvR\n",
    "Non resta che visualizzare i Bayes Desicion Boundaries. Per tale visualizzazione si è scelta una classifcazione One-vs-Rest, cioè il decision boundary di una classe è costruito eguagliando il determinant score della classe in esame e quello di tutti gli altri dati trattati come un'unica classe. Questo è utile per visualizzazioni multiclasse (più di 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('terrain')\n",
    "\n",
    "for l,c,m in zip(np.unique(y),[np.array([cmap(0)]),np.array([cmap(1/6)]),np.array([cmap(1/3)]),np.array([cmap(1/2)]),np.array([cmap(2/3)]),np.array([cmap(5/6)]),np.array([cmap(1)])],['s','x','o','s','x','o','s']):\n",
    "    scatter = plt.scatter(X[y==l,0], X[y==l,5], c=c, marker=m, label=le.classes_[l], alpha=1.00)\n",
    "    scatter.set_alpha(0.15);\n",
    "    plt.xlim(-2, 2);\n",
    "    plt.ylim(-2.5, 2);\n",
    "    plt.grid();\n",
    "\n",
    "x1 = np.array([np.min(X[:,0], axis=0), np.max(X[:,0], axis=0)])\n",
    "\n",
    "for i, c in enumerate([cmap(0), cmap(1/6),cmap(1/3),cmap(1/2),cmap(2/3),cmap(5/6),cmap(1)]):\n",
    "    b, w1, w2 = lda.intercept_[i], lda.coef_[i][0], lda.coef_[i][5];\n",
    "    y1 = -(b+x1*w1)/w2; \n",
    "    lines = plt.plot(x1,y1,c=c,alpha=1);\n",
    "    leg = plt.legend(handles=scatter.legend_elements()[0], labels=le.classes_[i], fontsize='large', title=\"Classes\");\n",
    "    plt.xlim(-2, 2);\n",
    "    plt.ylim(-2.5, 1.7);\n",
    "    plt.title('\\nDECISION BOUNDARIES - LDA\\n');\n",
    "\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f39b44",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machine\n",
    "La SVM è un algoritmo supervisionato di classificazione di nuovi dati che quindi richiede l'uso di un training set e di un test set.\n",
    "\n",
    "L'idea è quella di separare dei punti in uno spazio d-dimensionale con un **iperpiano**, cioè un sotto spazio piano affine (d-1)-dimensionale, a seconda della classe di appartenenza. Diviso lo spazio in tanti spazi quanti le classi, all'inserimento di un nuovo dato, per classificarlo basterà vedere la regione di appartenenza.\n",
    "\n",
    "L'algoritmo si occupa anche di cercare il miglior iperpiano che massimizza il **margine**, cioè il doppio della distanza minima dell'iperpiano dai punti di ogni classe. Inoltre, se non esiste tale iperpiano, si può settare una tolleranza. Nel caso il dataset non sia neanche \"quasi\" linearmente separabile, si passa all'utilizzo dei **kernel**, cioè delle proiezioni che generano delle regioni di appartenenza alle classi non poligonali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9100a44",
   "metadata": {},
   "source": [
    "### SVM: basi teoriche\n",
    "Consideriamo il caso di classificazione binaria (2 classi).\n",
    "Qualche notazione:\n",
    "1. $\\mathcal{T} = \\{(\\boldsymbol{x}_1, y_1), \\ldots , (\\boldsymbol{x}_T, y_T) \\}\\subset \\mathbb{R}^n\\times \\{\\pm 1\\}$ è il training set costiutito da $T$ coppie $(\\boldsymbol{x}_i, y_i)$, con $y_i=\\pm 1$ rappresentante la classe del vettore $\\boldsymbol{x_i}\\in\\mathbb{R}^n$. \n",
    "\n",
    "2. Indichiamo rispettivamente l'insieme dei vettori e l'insieme delle classi in $\\mathcal{T}$ con $$X_{\\mathcal{T}}=\\{\\boldsymbol{x}_1,\\ldots ,\\boldsymbol{x}_T\\},$$ $$Y_{\\mathcal{T}}=\\{y_1,\\ldots ,y_T\\}$$ gli insiemi dei vettori \n",
    "\n",
    "3. Indichiamo con $\\Pi_{\\boldsymbol{w},b}$ l'iperpiano di $\\mathbb{R}^n$ definito dal vettore normale $\\boldsymbol{w}\\in\\mathbb{R}^n$ e dal parametro $b\\in\\mathbb{R}$, cioè $$\\Pi_{\\boldsymbol{w},b}:= \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\boldsymbol{w}^\\top\\boldsymbol{x} + b = 0\\}\\,.$$\n",
    "\n",
    "4. Indichiamo con $\\mathrm{dist}(\\Pi_{\\boldsymbol{w},b}, \\boldsymbol{x})$ la distanza euclidea tra un vettore $\\boldsymbol{x}\\in\\mathbb{R}^n$ e l'iperpiano $\\Pi_{\\boldsymbol{w},b}$. In particolare, ricordiamo che $$\\mathrm{dist}(\\Pi_{\\boldsymbol{w},b}, \\boldsymbol{x}) = \\frac{|\\boldsymbol{w}^\\top\\boldsymbol{x} + b|}{||\\boldsymbol{w}||}\\,.$$\n",
    "\n",
    "5.  L'ampiezza del margine di un iperpiano è: $$M_{\\boldsymbol{w},b} = 2\\cdot \\min_{\\boldsymbol{x}}\\mathrm{dist}(\\Pi_{\\boldsymbol{w},b}, \\boldsymbol{x})\\,.$$\n",
    "\n",
    "Poiché per ogni scalare $k\\in\\mathbb{R}\\setminus\\{0\\}$ vale $\\Pi_{k\\boldsymbol{w},kb}\\equiv \\Pi_{\\boldsymbol{w}, b}$, possiamo restringere la ricerca dell'iperpiano separatore ottimale a quelli con parametri $\\boldsymbol{w}$ e $b$ tali che $$|\\boldsymbol{w}^\\top\\boldsymbol{x}+b|\\geq 1\\,, \\ \\forall \\, \\boldsymbol{x}\\in X_{\\mathcal{T}} \\,.$$ Questi sono detti **iperpiani canonici** rispetto $X_{\\mathcal{T}}$ e vettori $\\boldsymbol{x}\\in X_{\\mathcal{T}}$ tali che $|\\boldsymbol{w}^\\top\\boldsymbol{x}+b| = 1$ sono detti **support vectors**.\n",
    "\n",
    "Notiamo come se $\\Pi_{\\boldsymbol{w},b}$ è un iperpiano canonico, allora $M_{\\boldsymbol{w},b}=\\frac{2}{||\\boldsymbol{w}||}$ e che restringendo il problema agli iperpiani canonici, massimizzare $M_{\\boldsymbol{w},b} =\\frac{2}{||\\boldsymbol{w}||}$ è equivalente a minimizzare $\\frac{1}{2}||\\boldsymbol{w}||^{2} = \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w}$, da cui:\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\min_{\\boldsymbol{w}} \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w}\\\\\n",
    "        y_i (\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) \\geq 1 \\,,\\quad \\forall \\ i=1,\\ldots ,T\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Passiamo al problema duale:\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\min_{\\boldsymbol{\\alpha}} \\frac{1}{2}\\boldsymbol{\\alpha}^\\top Q \\boldsymbol{\\alpha} - \\sum_{i=1}^T \\alpha_i\\\\\n",
    "        \\sum_{i=1}^T \\alpha_i y_i = 0\\\\\n",
    "        \\alpha_i \\geq 0\\,,\\quad \\forall \\ i=1,\\ldots ,T\n",
    "    \\end{cases}\\,,\n",
    "\\end{equation}\n",
    "dove $Q = \\left(q_{i,j}\\right)_{i,j=1,\\ldots ,T} =  \\left( y_iy_j\\boldsymbol{x}_i^\\top\\boldsymbol{x}_j \\right)_{i,j=1,\\ldots ,T}\\,$.\n",
    "\n",
    "I dati in analisi sono (altamente) affetti da rumore quindi va adoperato una Soft Margin SVM. Riformuliamo di conseguenza il problema.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\min_{\\boldsymbol{w}} \\frac{1}{2} \\left( \\boldsymbol{w}^\\top\\boldsymbol{w} + C \\sum_{i=1}^T \\xi_i^2 \\right)\\\\\n",
    "        y_i (\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) \\geq 1 - \\xi_i \\,,\\quad & \\forall \\ i=1,\\ldots ,T\\\\\n",
    "        \\xi_i\\geq 0\\,,\\quad & \\forall \\ i=1,\\ldots ,T\\\\\n",
    "    \\end{cases}\\,;\n",
    "\\end{equation}\n",
    "\n",
    "Il parametro $C\\in\\mathbb{R}^+$ è un **parametro di regolarizzazione** che caratterizza il rilassamento delle condizioni per il margine:\n",
    "- $C\\rightarrow 0$ aumenta la \"*morbidezza*\" del margine, permettendo ai vettori $\\boldsymbol{x}_i$ di superarlo illimitatamente;\n",
    "- $C\\rightarrow +\\infty$ aumenta la \"*durezza*\" del margine, permettendo ai vettori $\\boldsymbol{x}_i$ di superarlo impercettibilmente;\n",
    "\n",
    "Il dataset in analisi inoltre ha classi fortemente non linearmente separabili, quindi è conviente l'uso dei Kernel. L'idea è qurlla di proiettare le istanze in uno spazio di dimensione maggiore dove sono quasi linearmente separabili, classificarle e riproiettarle. Quindi scegliamo una mappa (tipicamente _non lineare_) arbitraria\n",
    "$$\\phi :\\mathbb{R}^n\\rightarrow \\mathbb{R}^m\\,, \\quad \\text{con }m>n\\,,$$\n",
    "e risolviamo il problema con una SVM rispetto al _nuovo training set_ \n",
    "$\\mathcal{T}^\\phi = \\{ \\left(\\boldsymbol{\\varphi}_1,y_1\\right),\\ldots , \\left(\\boldsymbol{\\varphi}_T),y_T\\right)\\}\\in\\mathbb{R}^m\\times \\{\\pm 1\\}$ con $\\boldsymbol{\\varphi}_i = \\phi(\\boldsymbol{x}_i),\\, \\forall \\ i=1,\\ldots ,T\\,, $ \"sperando\" che ora le classi siano diventate _linearmente separabili_.\n",
    "\n",
    "Chiaramente ciò comporta un considerevole aumento dei costi computazionali e un possibile fallimento nella classificazione lineare. Per ovviare a ciò si usa il **Kernel Trick**.\n",
    "Sia $\\mathcal{X}\\subseteq\\mathbb{R}^n$ il dominio dei vettori $\\boldsymbol{x}$, quindi un kernel è nella forma $k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "Per cui esiste un'unica mappa $\\phi:\\mathcal{X}\\rightarrow\\mathcal{H}\\subseteq\\mathbb{R}^m$, con $m>n$ ed $\\mathcal{H}$ spazio di _Hilbert_, per cui il prodotto scalare in $\\mathcal{H}$ di $\\boldsymbol{\\varphi}_i=\\phi(\\boldsymbol{x}_i)$ e $\\boldsymbol{\\varphi}_j=\\phi(\\boldsymbol{x}_j)$, per ogni $\\boldsymbol{x}_i,\\boldsymbol{x}_j\\in\\mathcal{X}$, è definito da $k$; in altre parole: $$\\langle\\boldsymbol{\\varphi}_i,\\boldsymbol{\\varphi}_j\\rangle_{\\mathcal{H}} = k(\\boldsymbol{x}_i,\\boldsymbol{x}_j).$$\n",
    "I vantaggi di tale \"trucco\" sono due: parliamo di prodotti scalari in $\\mathbb{R}^m$ che possono essere calcolati in $\\mathbb{R}^n$, quindi si può scegliere un qualsiasi $m>n$ senza appesantire l'algoritmo; non è necessario conoscere $\\phi$, ma solo $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be40c9d",
   "metadata": {},
   "source": [
    "### SVM: Grid Search\n",
    "\n",
    "Scegliamo i parametri C, $\\gamma$ e grado che danno la maggiore precisione nell'uso dei seguenti algoritmi: LinearSVC, SVC con kernel lineare, rbf, polinomiale. Per far questo usiamo la **Grid Search**.\n",
    "Significato dei parametri:\n",
    "\n",
    "- **C**: aggiunge una penalità ogni volta si incorre in una classificazione errata. Perciò un basso valore di C ha più oggetti mal classificati, ma un alto valore di C potrebbe causare overfitting;\n",
    "- **$\\gamma$**: determina l'influenza dei punti di training sulla classificazione. Per un basso valore di $\\gamma$, tutti i punti di training avrebbero influenza, un alto valore di $\\gamma$ metterebbe in luce solo i punti vicini al decision boundary, potenzialmnte causando overfitting;\n",
    "- **degree**: indica il grado del polinomio usato nel kernel polinomiale.\n",
    "\n",
    "Per prima cosa dividiamo il dataset in training, test e validation set (rispettivamente: 30%, 50%, 20% del dataset)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "276ecb6e",
   "metadata": {},
   "source": [
    "acc = pd.DataFrame(index=['Linear Kernel','LinearSVC','RBF Kernel','3Poly Kernel'], columns=['C=0.0001','C=0.001','C=0.01','C=0.1','C=1','C=10','C=100','C=1000'])\n",
    "cont = 0;\n",
    "for C in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    models = [svm.SVC(kernel='linear', C=C),\n",
    "              svm.LinearSVC(C=C, max_iter=10000),\n",
    "              svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "              svm.SVC(kernel='poly', degree=3, gamma='auto', C=C)]\n",
    "    models= [clf.fit(X_train, y_train) for clf in models]\n",
    "    #predictions = [clf.predict(X_test) for clf in models]\n",
    "    acc.iloc[:,cont] = [clf.score(X_test, y_test) for clf in models]\n",
    "    cont = cont+1;\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(13611)\n",
    "random_state = 20210520\n",
    "test_p = 0.3 + 0.2;\n",
    "val_p = 0.2 / 0.5;\n",
    "ind_train, ind_test = train_test_split(indices, test_size=test_p, random_state=random_state, shuffle=True)\n",
    "ind_train, ind_val = train_test_split(ind_train, test_size=val_p, random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136219e0",
   "metadata": {},
   "source": [
    "Successivamente si crea una griglia tridimensionale dove ad ogni punto sono associati tre valori di C, $\\gamma$ e degree presi dalle liste in input. La funzione GridSearchCV, grazie ai test sul validation set, permette di sottoporre ogni set di parametri ad una lista di algoritmi kernel SVM in input e conseguentemente di stilare una classifica di questi in base all'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'degree': [2.0, 3.0],\n",
    "              'kernel': ['rbf','poly','linear']};\n",
    "\n",
    "grid = GridSearchCV(estimator=svm.SVC(class_weight='balanced'), param_grid=param_grid, verbose=0, scoring='f1_weighted',\n",
    "                      return_train_score=True, cv=zip([ind_train], [ind_val]));\n",
    "grid.fit(X_beans_scaled, beans['Class'].values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad21069",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)\n",
    "grid_predictions = grid.predict(X_test);\n",
    "df_results = pd.DataFrame(grid.cv_results_)\n",
    "df_results = df_results.sort_values(['rank_test_score'], ascending=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961b833",
   "metadata": {},
   "source": [
    "Da qui possiamo vedere come il migliore tra gli algoritmi analizzati è una SVM con un kernel rbf e parametri C=100 e $\\gamma$=0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c161e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.iloc[:,4:7] = df_results.iloc[:,4:7].astype(float);\n",
    "to_plot_par = pd.DataFrame(columns=['linear','rbf','poly'], index=['C','Gamma','degree']);\n",
    "\n",
    "best = df_results[df_results.param_kernel =='linear'].head(1);\n",
    "to_plot_par.iloc[0,0] = best.iloc[0,4];\n",
    "to_plot_par.iloc[1,0] = best.iloc[0,6];\n",
    "\n",
    "best = df_results[df_results.param_kernel =='poly'].head(1);\n",
    "to_plot_par.iloc[0,2] = best.iloc[0,4];\n",
    "to_plot_par.iloc[1,2] = best.iloc[0,4];\n",
    "to_plot_par.iloc[2,2] = best.iloc[0,5];\n",
    "\n",
    "best = df_results[df_results.param_kernel =='rbf'].head(1);\n",
    "to_plot_par.iloc[0,1] = best.iloc[0,4];\n",
    "to_plot_par.iloc[1,1] = best.iloc[0,6];\n",
    "\n",
    "to_plot_par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b8dc1",
   "metadata": {},
   "source": [
    "Per scopi illustrativi, i migliori parametri anche per una SVM con kernel lineare e per una SVM con kernel polinomiale sono riportati qui sopra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2ecc1",
   "metadata": {},
   "source": [
    "### SVM: Visualizzazione\n",
    "\n",
    "Andiamo quindi a visualizzare i plot dei modelli studiati con i migliori iperparametri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b146cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = np.zeros((Z2p[:,0].shape[0], 2))\n",
    "for i in range (Z2p[:,0].shape[0]):\n",
    "    X[i,0] = Z2p[i,0];\n",
    "    X[i,1] = Z2p[i,1];\n",
    "y = beans['Class'];\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "\n",
    "models = (svm.SVC(kernel='linear', C=to_plot_par.iloc[0,0]),\n",
    "          svm.LinearSVC(C=to_plot_par.iloc[0,0], max_iter=10000),\n",
    "          svm.SVC(kernel='rbf', gamma=to_plot_par.iloc[1,1], C=to_plot_par.iloc[0,1]),\n",
    "          svm.SVC(kernel='poly', degree=to_plot_par.iloc[2,2], gamma=to_plot_par.iloc[1,2], C=to_plot_par.iloc[0,2]))\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = ('\\nSVC with linear kernel\\n',\n",
    "          '\\nLinearSVC (linear kernel)\\n',\n",
    "          '\\nSVC with RBF kernel\\n',\n",
    "          '\\nSVC with polynomial (degree 3) kernel\\n')\n",
    "\n",
    "cmap = plt.cm.coolwarm\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy, cmap=cmap, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=cmap, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('\\nPC1: Dimension\\n')\n",
    "    ax.set_ylabel('\\nPC2: Roundness\\n')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192683d",
   "metadata": {},
   "source": [
    "<img src=\"IMG/SVM_over_PCA.JPG\" width=\"960\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ab662",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "Possiamo vedere come l'algoritmo di classificazione più accurato è la Support Vector Machine con Kernel rbv e iperparametri C=100, $\\gamma$=0.01 per una accuracy score finale del 93,0488%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
